blocks: <p>E.T. Jaynes united Bayesian Probability Theory with Information Theory.</p><p>From a <a class="url http outside" href="http://library.wustl.edu/units/spec/exhibits/crow/jaynesbio.html">brief biography</a>:<br /></p><blockquote><p>Ed's vastly influential synthesis of the ideas and results of Laplace, Bayes, Jeffreys, Cox, and Shannon into a consistent modern framework of probabilistic reasoning is a natural outgrowth of this striking early work. A series of beautifully composed and argued papers on this subject and on information-theoretic statistical mechanics was published in conference proceedings volumes -- it was commonplace for mainstream journals to reject Ed's manuscripts. Among these is the classic "How the Brain Does Plausible Reasoning," originally a 1959 Stanford Microwave Laboratory Report. Jaynes' impact on the field of statistical inference has been enormous and has been summarized in Probability &amp; Physics: Essays in Honor of Edwin T. Jaynes, edited by W. T. Grandy, Jr. and P. W. Millonni and published by Cambridge University Press in 1993. Ed left us a virtually complete book manuscript entitled Probability Theory: The Logic of Science, a monumental contribution which, on-line and in preprint form, has already become one of the most widely studied books in science. His writings expose the foundations of "the calculus of inductive reasoning" with a clarity and elegance that will continue to enlighten and delight his readers for many generations to come.<br /></p></blockquote><p>Jaynes is probably best known for his contribution to the Principle of Maximum Entropy. Writing first in the Physical Review in <a class="url http outside" href="http://bayes.wustl.edu/etj/articles/theory.1.pdf">1957</a>, he had decades in which to simplify and clarify the arguments when he summarized them in <a class="url http outside" href="http://bayes.wustl.edu/etj/articles/rational.pdf">1982</a>. Apply the Stirling approximation to the factorials you get calculating multiplicities and voila you get Shannon's familiar -sum (p log p). One sees readily why an overwhelming fraction of outcomes are the high entropy outcomes, and therefore why, for example, virtually every common probability distribution, such as the uniform, exponential, geometric, normal, etc., is the unique maximum entropy member of some class of distributions, subject to some constraint. And the use of Lagrange Multipliers generally offers a very straightforward way to calculate such solutions.</p><p>As a physicist (whose advisor was Wigner), Jaynes was particularly obsessed with widely held conceptual blunders. From the same biography linked above:<br /></p><blockquote><p>Ed insisted that some of the thorniest conceptual problems faced in physics, notably in statistical physics and quantum theory, arise from a mistaken identification of probabilities as physical quantities rather than as representations of the available information on a system -- a confusion between what is ontological and what is epistemological. Like Einstein, he was repelled by the Copenhagen interpretation of quantum mechanics and what he viewed as an incursion of mysticism into science.<br /></p></blockquote><p>For a remarkably lucid look at three such thorny problems in physics, see the first 16 pages of his 1988 paper <a class="url http outside" href="http://bayes.wustl.edu/etj/articles/cmystery.pdf">Clearing up Mysteries: The Original Goal</a>.</p><hr /><p>Links:</p><p><a class="url http outside" href="http://bayes.wustl.edu/etj/node1.html">Published works</a>.</p><p><a class="url http outside" href="http://www.naturalthinker.net/trl/texts/Science/Jaynes,%20E.%20T.%20-%20Probability%20Theory%20-%20The%20Logic%20Of%20Science.pdf">Probability Theory: The Logic of Science</a> as a single pdf file.</p><p>The <a class="url http outside" href="http://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes">entry</a> on E.T. Jaynes at Wikipedia.</p>
lastmajorsummary: Rollback to 2018-06-03 10:40 UTC
ip: 108.48.88.208
diff-major: 1
summary: Rollback to 2018-06-03 10:40 UTC
text: E.T. Jaynes united Bayesian Probability Theory with Information Theory.
	
	From a [http://library.wustl.edu/units/spec/exhibits/crow/jaynesbio.html brief biography]:
	"""
	Ed's vastly influential synthesis of the ideas and results of Laplace, Bayes, Jeffreys, Cox, and Shannon into a consistent modern framework of probabilistic reasoning is a natural outgrowth of this striking early work. A series of beautifully composed and argued papers on this subject and on information-theoretic statistical mechanics was published in conference proceedings volumes -- it was commonplace for mainstream journals to reject Ed's manuscripts. Among these is the classic "How the Brain Does Plausible Reasoning," originally a 1959 Stanford Microwave Laboratory Report. Jaynes' impact on the field of statistical inference has been enormous and has been summarized in Probability & Physics: Essays in Honor of Edwin T. Jaynes, edited by W. T. Grandy, Jr. and P. W. Millonni and published by Cambridge University Press in 1993. Ed left us a virtually complete book manuscript entitled Probability Theory: The Logic of Science, a monumental contribution which, on-line and in preprint form, has already become one of the most widely studied books in science. His writings expose the foundations of "the calculus of inductive reasoning" with a clarity and elegance that will continue to enlighten and delight his readers for many generations to come.
	"""
	
	Jaynes is probably best known for his contribution to the Principle of Maximum Entropy.  Writing first in the Physical Review in [http://bayes.wustl.edu/etj/articles/theory.1.pdf 1957], he had decades in which to simplify and clarify the arguments when he summarized them in [http://bayes.wustl.edu/etj/articles/rational.pdf 1982]. Apply the Stirling approximation to the factorials you get calculating multiplicities and voila you get Shannon's familiar -sum (p log p).  One sees readily why an overwhelming fraction of outcomes are the high entropy outcomes, and therefore why, for example, virtually every common probability distribution, such as the uniform, exponential, geometric, normal, etc., is the unique maximum entropy member of some class of distributions, subject to some constraint. And the use of Lagrange Multipliers generally offers a very straightforward way to calculate such solutions.
	
	As a physicist (whose advisor was Wigner), Jaynes was particularly obsessed with widely held conceptual blunders. From the same biography linked above:
	"""
	Ed insisted that some of the thorniest conceptual problems faced in physics, notably in statistical physics and quantum theory, arise from a mistaken identification of probabilities as physical quantities rather than as representations of the available information on a system -- a confusion between what is ontological and what is epistemological. Like Einstein, he was repelled by the Copenhagen interpretation of quantum mechanics and what he viewed as an incursion of mysticism into science.
	"""
	
	For a remarkably lucid look at three such thorny problems in physics, see the first 16 pages of his 1988 paper [http://bayes.wustl.edu/etj/articles/cmystery.pdf Clearing up Mysteries: The Original Goal].
	
	----
	Links:
	
	[http://bayes.wustl.edu/etj/node1.html Published works].
	
	[http://www.naturalthinker.net/trl/texts/Science/Jaynes,%20E.%20T.%20-%20Probability%20Theory%20-%20The%20Logic%20Of%20Science.pdf Probability Theory: The Logic of Science] as a single pdf file.
	
	The [http://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes entry] on E.T. Jaynes at Wikipedia.
	
languages: 
lastmajor: 5
diff-minor: <p><strong>Changed:</strong></p>
	<div class="old"><p>&lt; <strong class="changes">DeletedPage</strong></p></div><p><strong>to</strong></p>
	<div class="new"><p>&gt; <strong class="changes">E.T. Jaynes united Bayesian Probability Theory with Information Theory.<br />&gt; From a [http://library.wustl.edu/units/spec/exhibits/crow/jaynesbio.html brief biography]:<br />&gt; """<br />&gt; Ed's vastly influential synthesis of the ideas and results of Laplace, Bayes, Jeffreys, Cox, and Shannon into a consistent modern framework of probabilistic reasoning is a natural outgrowth of this striking early work. A series of beautifully composed and argued papers on this subject and on information-theoretic statistical mechanics was published in conference proceedings volumes -- it was commonplace for mainstream journals to reject Ed's manuscripts. Among these is the classic "How the Brain Does Plausible Reasoning," originally a 1959 Stanford Microwave Laboratory Report. Jaynes' impact on the field of statistical inference has been enormous and has been summarized in Probability &amp; Physics: Essays in Honor of Edwin T. Jaynes, edited by W. T. Grandy, Jr. and P. W. Millonni and published by Cambridge University Press in 1993. Ed left us a virtually complete book manuscript entitled Probability Theory: The Logic of Science, a monumental contribution which, on-line and in preprint form, has already become one of the most widely studied books in science. His writings expose the foundations of "the calculus of inductive reasoning" with a clarity and elegance that will continue to enlighten and delight his readers for many generations to come.<br />&gt; """<br />&gt; Jaynes is probably best known for his contribution to the Principle of Maximum Entropy.  Writing first in the Physical Review in [http://bayes.wustl.edu/etj/articles/theory.1.pdf 1957], he had decades in which to simplify and clarify the arguments when he summarized them in [http://bayes.wustl.edu/etj/articles/rational.pdf 1982]. Apply the Stirling approximation to the factorials you get calculating multiplicities and voila you get Shannon's familiar -sum (p log p).  One sees readily why an overwhelming fraction of outcomes are the high entropy outcomes, and therefore why, for example, virtually every common probability distribution, such as the uniform, exponential, geometric, normal, etc., is the unique maximum entropy member of some class of distributions, subject to some constraint. And the use of Lagrange Multipliers generally offers a very straightforward way to calculate such solutions.<br />&gt; As a physicist (whose advisor was Wigner), Jaynes was particularly obsessed with widely held conceptual blunders. From the same biography linked above:<br />&gt; """<br />&gt; Ed insisted that some of the thorniest conceptual problems faced in physics, notably in statistical physics and quantum theory, arise from a mistaken identification of probabilities as physical quantities rather than as representations of the available information on a system -- a confusion between what is ontological and what is epistemological. Like Einstein, he was repelled by the Copenhagen interpretation of quantum mechanics and what he viewed as an incursion of mysticism into science.<br />&gt; """<br />&gt; For a remarkably lucid look at three such thorny problems in physics, see the first 16 pages of his 1988 paper [http://bayes.wustl.edu/etj/articles/cmystery.pdf Clearing up Mysteries: The Original Goal].<br />&gt; ----<br />&gt; Links:<br />&gt; [http://bayes.wustl.edu/etj/node1.html Published works].<br />&gt; [http://www.naturalthinker.net/trl/texts/Science/Jaynes,%20E.%20T.%20-%20Probability%20Theory%20-%20The%20Logic%20Of%20Science.pdf Probability Theory: The Logic of Science] as a single pdf file.<br />&gt; The [http://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes entry] on E.T. Jaynes at Wikipedia.</strong></p></div>
flags: 0
ts: 1528057710
minor: 0
username: z
host: 166.137.240.115
revision: 5
keep-ts: 1528057710
